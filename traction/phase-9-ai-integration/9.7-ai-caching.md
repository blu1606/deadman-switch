# 9.7 AI Semantic Caching (pgvector)

> **Goal:** 0ms latency for repeated/similar AI questions using Vector Embeddings  
> **Tech:** Supabase `pgvector` + Vercel AI SDK  
> **Updated:** 2025-12-11 (Promoted from Future Plan)

---

## üéØ Concept: "Fuzzy" Caching

Traditional hash caching only hits if the prompt is **identical**.
*   User A: "Help me write a goodbye letter"
*   User B: "Assist me in drafting a farewell note"

**Hash Cache:** MISS (Slow) ‚ùå
**Semantic Cache:** HIT (Instant) ‚úÖ

We use **Embedding Vectors** to understand that these two prompts mean the same thing.

---

## üìê Architecture

1.  **Incoming Prompt** -> Generate Embedding (via OpenAI/Gecko/etc).
2.  **Vector Search** -> Query Supabase `ai_cache` for nearest neighbor.
3.  **Threshold Check** -> If similarity > 0.95 (Very close), return cached response.
4.  **Fallback** -> If no match, call LLM -> Generate Response -> Store Embedding.

---

## üóÑÔ∏è Database Schema Updates

Requires `vector` extension.

```sql
-- Enable extension
create extension if not exists vector;

-- Update Cache Table
alter table ai_cache 
add column if not exists embedding vector(768); -- 768 for Google GenAI / Vertex

-- Vector Index (IVFFlat for speed)
create index on ai_cache using ivfflat (embedding vector_cosine_ops)
with (lists = 100);
```

---

## üõ†Ô∏è Implementation Plan

### 1. Vector Generation Utiltity
`lib/ai/embedding.ts`
```ts
import { google } from '@ai-sdk/google';
import { embed } from 'ai';

export async function generateEmbedding(text: string) {
  const { embedding } = await embed({
    model: google.textEmbeddingModel('text-embedding-004'),
    value: text,
  });
  return embedding;
}
```

### 2. Cache Lookup (Postgres Function)
Create a stored procedure for fast similarity search.

```sql
create or replace function match_cached_response (
  query_embedding vector(768),
  match_threshold float,
  match_count int
)
returns table (
  id uuid,
  response text,
  similarity float
)
language plpgsql
as $$
begin
  return query
  select
    ai_cache.id,
    ai_cache.response,
    1 - (ai_cache.embedding <=> query_embedding) as similarity
  from ai_cache
  where 1 - (ai_cache.embedding <=> query_embedding) > match_threshold
  order by ai_cache.embedding <=> query_embedding
  limit match_count;
end;
$$;
```

### 3. Integration flow
In `api/generate-hint/route.ts`:

```ts
// 1. Generate embed
const embedding = await generateEmbedding(prompt);

// 2. Check cache
const { data: cached } = await supabase.rpc('match_cached_response', {
  query_embedding: embedding,
  match_threshold: 0.92, // High confidence only
  match_count: 1
});

if (cached && cached.length > 0) {
  return new Response(cached[0].response); // ‚ö° INSTANT
}

// 3. Call LLM (Slow)
const result = await streamText({...});
// ... save result + embedding to DB async ...
```

---

## ‚úÖ Checklist

- [ ] Enable `vector` extension in Supabase
- [ ] Add `embedding` column to `ai_cache`
- [ ] Create `match_cached_response` RPC function
- [ ] Implement `generateEmbedding` (Google models are cheap/fast)
- [ ] Integrate into AI processing pipeline

---

## ‚ö†Ô∏è Risks & Mitigations

- **False Positives:** Returning a "Write Will" response for a "Write Poem" prompt.
    - *Mitigation:* Set high threshold (0.92+) and include `prompt_type` filter in the query.
- **Cost:** Embedding generation costs money?
    - *Mitigation:* Google `text-embedding-004` is extremely cheap (virtually free for low volume).
